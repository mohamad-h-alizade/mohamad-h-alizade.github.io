<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
   "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
   <head>
      <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
      <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
      <link rel="stylesheet" href="jemdoc.css" type="text/css" />
      <title>Mohamad H. Alizade</title>
   </head>
   <body>
      <table summary="Table for page layout." id="tlayout">
         <tr valign="top">
            <td id="layout-menu">
               <div class="menu-item"><a href="index.html">Home</a></div>
               <div class="menu-item"><a href="research.html">Research</a></div>
               <div class="menu-item"><a href="courses.html" class="current">Courses</a></div>
               <div class="menu-item"><a href="about.html">About Me</a></div>
               <div class="menu-item"><a href="./documents/Alizade-CV.pdf">CV</a></div>

<!--               <div class="menu-category">Research</div>
               <div class="menu-item"><a href="experience.html">Experience</a></div>
               <div class="menu-item"><a href="Projects.html">Projects</a></div>
-->
            </td>
            <td id="layout-content">
               <div id="toptitle">
                  <h1>Selected Courses</h1>
               </div>
               <p>Here's a summary of some selective courses I took.</p>
               <h2>Machine Learning</h2>
               <p>Through this course, I paved my way toward a formal understanding of learning
theory. PAC learning introduced generalization guarantees and VC-dimension expressed the
representational capacity of function sets. The neat idea was the link that connected these two
concepts to form the fundamental theorem of learning. Furthermore, the time and sample complexity
of many learning algorithms were covered and we get used to utilizing probabilistic inequalities like
Chebyshev’s and Hoeffding's. By the end, witnessing the intricate interplay of model sparsity,
robustness, and scalability - and observing them meticulously unifying and conceptually converging -
expanded my intellectual philosophy and motivated me to pursue a graduate education.</p>
               <h2>Tensor Decompositions</h2>
               <p>In this course, I ranked 1st among all undergraduate students.
Tensor decompositions reviewed linear algebra, block matrix identities, Khatri-Rao
product in vectorization, and then discussed common matrix factorization methods like LU and SVD.
Further in the course, we built on these notions and applied them to analyze unfolded tensors. We
implemented algorithms for Tucker decomposition, and here, I realized how rigorous mathematical
objects offer an algorithmic framework to bridge the gap between theory and practice. The works of
Dr. Cischocki were mostly discussed and we researched low-rank tensor completion and
dimensionality reduction. In the application domain, we applied them to EEG signals and measured
their effectiveness in brain-computer interfaces (BCI)</p>
               <h2>Convex Optimization</h2>
               <p>
               A course based on the "Convex Optimization: Theory and Application" by Boyd. We covered the first four theoretical chapters plus some algorithms and their convergence guarantees.
               The course initially focused on recognizing convex (and quasi-convex) problems via transformations on sets and functions that keep the convexity. Later on the primal and dual problems were introduced and we learned how to certify a solver. Even though the course mostly highlighted the theoretical aspects of convex optimization, I personlly gained hands-on experience by experimenting with CVXPY library.
               </p>
               <h2>AI and Biomedical Computations</h2>
               <p>The course should have been named computational intelligence. Here we delved into various biologically inspired paradigms on soft computing such as: neural networks, evolutionary algorithms, and fuzzy systems. The course focused on the principles of artificial neural networks, including the implementation and understanding of backpropagation. We first started with the perceptron model and after discussing its limitations we covered multilayer feed-forward networks and showed their vast representational power in approximating functions. The course also introduced us to diverse strategies for non-convex optimization problems such as ant colony and particle swarm optimization. Additionally, the course covered fuzzy systems, which are crucial in handling the concept of partial truth. We used this idea to develop control policies using soft descriptions. The capstone project involved classifying brain EEG motor imagery signals, applying theoretical knowledge to a practical biomedical problem. As a result, we utilized concepts in computational intelligence to analyze EEG data and classify different types of motor imagery signals. I recieved the highest score in the project.</p>
               <h2>Random Processes</h2>
               <p>Random processes expanded upon my probability and statistics coursework. We covered
               chapters from Papoulis and Ross’s textbook and discussed stationary random processes, Markov
               chains and graphical models. The experience offered me a simplified overview of page ranking
               systems and we took a peek at causal inference. Furthermore, Gibbs sampling was analyzed in the
               context of Boltzmann machines which in return exposed me to some ideas in statistical learning.</p>
               <h2>Medical Imaging Systems</h2>
               <p> This course focused on various imaging modalities, including CT, X-ray, and MRI. The ideas in signal processing played a crucial role in these imaging techniques, enabling the meaningful transformation of acquired signals into images. A fascinating idea explored in the course was the connection between line integrals and Fourier transform in CT imaging. Linking these concepts allowed for the reconstruction of tomographic images from X-ray data. Later on, the focus shifted to discussing inverse problems. We explored computational limitations and various trade-offs of these problems, highlighting the challenges faced in the field of medical imaging and the importance of convex optimization techniques.</p>
                <h2>Image Processing</h2> 
                <p>
                    The course covered the Digital Image Processing textbook by C. Gonzalez. we delved into topics such as intensity transformations, spatial filtering, frequency domain enhancement, multiresolution processing, and image restoration. Further, we explored color and morphological image processing, image compression, and geometric transformations. At the end, the course discussed various segmentation and blending techniques. The homeworks let us implement many algorithms such as poison and pyramid blending, active contours, and texture generation. 
                </p>
                <h2>Control Theory</h2>
                <p>
                I took three selective courses on control theory:
                <ul>
                    <li><b>Modern Control:</b> I ranked 1st in the class. Course highly depended on linear algebra to analyze dynamical systems, design state feedback controllers, and establish observability/controllability of systems.</li>
                    <li><b>Industrial Control:</b> I ranked 1st in the class. We mostly discussed system identification theory.</li>
                    <li><b>Digital Control:</b> I ranked 2nd in the class. As the name suggests, the course covered sampling, discretization, z-transform, and mixed systems that contain both continuous and digital signals.</li>
                </ul>
                </p>
               <div id="footer">
                  <div id="footer-text">
                  </div>
               </div>
            </td>
         </tr>
      </table>
   </body>
</html>

